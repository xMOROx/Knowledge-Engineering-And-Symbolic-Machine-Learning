# Slajd 1

## Plato: Inteligentne Roboty Robocode

**Wyzwanie:** Jak nauczyƒá robota Robocode skutecznej strategii walki bez rƒôcznego programowania z≈Ço≈ºonych regu≈Ç?

**Koncepcja:** Zastosowanie **Uczenia ze Wzmocnieniem (Reinforcement Learning)** w architekturze rozproszonej.

* Robot (Agent) uczy siƒô metodƒÖ pr√≥b i b≈Çƒôd√≥w w symulatorze Robocode.
* Centralny Serwer RL analizuje do≈õwiadczenia i trenuje "m√≥zg" robota.
* Orkiestrator zarzƒÖdza ca≈Çym procesem treningu.

---

# Slajd 2

## Architektura Systemu Plato

Szczeg√≥≈Çowy diagram przedstawiajƒÖcy komponenty i przep≈Çyw danych.

```mermaid
graph LR
    subgraph Orchestrator [Orchestrator üêç]
        direction TB
        O[train.py] -- Reads --> C[üìÑ config.yaml]
        O -- Manages --> P[ProcessManager]
        P -- Starts/Stops --> SP((üß† RL Server Process))
        P -- Starts/Stops --> RP((ü§ñ Robocode Process))
        P -- Starts/Stops --> TBP((üìä TensorBoard Process))
        O -- Triggers --> MVN[üõ†Ô∏è Maven Build]
    end

    subgraph RLServer [RL Server üêç]
        direction TB
        SP -- Contains --> Main[main.py]
        Main -- Creates --> ES[üì® Environment Server - UDP]
        Main -- Creates --> WS[‚öñÔ∏è Weight Server - HTTP]
        Main -- Uses --> QNet[üß† Q-Network - PyTorch]
        Main -- Uses --> Mem[üíæ Replay Memory]
        Main -- Uses --> Opt[‚öôÔ∏è Optimizer]
        Main -- Uses --> TBW[‚úçÔ∏è TensorBoard Writer]

        ES -- Stores --> Mem
        Mem -- Provides Sample --> QNet
        QNet -- Trained By --> Opt
        Opt -- Updates --> QNet
        QNet -- Logs Metrics --> TBW
        Mem -- Logs Size --> TBW
        QNet -- Saves --> ONNX[(üìÑ network_weights.onnx)]
        QNet -- Saves --> PT[(üíæ network_weights.pt)]
        WS -- Reads & Serves --> ONNX
        WS -- Reads & Serves --> UPDATES[(üìÑ updates.txt)]
        QNet -- Saves --> UPDATES
    end

    subgraph Environment [Environment ‚òï]
        direction TB
        RP -- Contains --> Sim[üïπÔ∏è Robocode Simulator]
        Sim -- Runs --> Agent[ü§ñ PlatoRobot Agent]
        Agent -- Uses --> NetDJL[üß† Network]
        Agent -- Uses --> StateRep[üì° StateReporter - UDP]
        Agent -- Observes --> GameState[üéÆ Game State]
        GameState --> Agent
        NetDJL -- Provides Policy --> Agent
        Agent -- Performs Action --> Sim
        Sim -- Provides Reward/Events --> Agent
    end

    subgraph Monitoring [Monitoring - via Browser]
       TBP -- Visualizes Data From --> TBW
    end


    %% Data Flows %%
    StateRep -- Experience (S,A,R,S',T) --> ES
    NetDJL -- Requests Weights (.onnx) --> WS
    MVN -- Produces --> RobotJAR[(üì¶ plato-robot.jar + deps)]
    RobotJAR -- Loaded By --> RP

 %% Component Style Definitions (Lighter Strokes for Dark Mode) %%
 classDef python fill:#cde4f1,stroke:#77b6e3,stroke-width:2px,color:#000;  
 classDef java fill:#fde3c0,stroke:#ffcc80,stroke-width:2px,color:#000;  
 classDef file fill:#f5f5f5,stroke:#e0e0e0,stroke-width:1px,stroke-dasharray: 4 4,color:#000; 
 classDef process fill:#d4f1f4,stroke:#7ededf,stroke-width:2px,color:#000; 
 classDef build fill:#e0e0e0,stroke:#bdbdbd,stroke-width:2px,color:#000; 
 classDef monitoring fill:#e8dff5,stroke:#ba68c8,stroke-width:2px,color:#000; 


 %% Apply Style Classes %%
 class Orchestrator,O,C,P python;
 class RLServer,Main,ES,WS,QNet,Mem,Opt,TBW python;
 class Environment,Sim,Agent,NetDJL,StateRep,GameState java;
 class Monitoring monitoring;
 class TBP process;
 class ONNX,PT,UPDATES,RobotJAR file;
 class SP,RP process;
 class MVN build;

```

---

# Slajd 3

## Opis Architektury

* **Orchestrator (Python):** Kontroluje ca≈Ço≈õƒá. `train.py` czyta `config.yaml` i u≈ºywa `ProcessManager` do zarzƒÖdzania procesami: Robocode (`RP`), Serwera RL (`SP`) i TensorBoard (`TBP`). Inicjuje te≈º budowanie (`Maven Build`).
* **RL Server (Python):** Uruchomiony jako proces (`SP`). `main.py` tworzy serwer UDP (`ES`) do odbierania do≈õwiadcze≈Ñ i serwer HTTP (`WS`) do udostƒôpniania wag `.onnx`. U≈ºywa `PyTorch` do trenowania sieci Q (`QNet`) na danych z Pamiƒôci Powt√≥rek (`Mem`), aktualizujƒÖc wagi za pomocƒÖ `Optimizera`. Zapisuje metryki (`TBW`) i modele (`.onnx`, `.pt`, `updates.txt`).
* **Environment (Java/Robocode):** Proces Robocode (`RP`) uruchamia agenta (`PlatoRobot Agent`). Agent u≈ºywa sieci neuronowej (`NetDJL` - ≈Çadowanej z `.onnx`) do podejmowania decyzji na podstawie stanu gry (`GameState`). Wysy≈Ça do≈õwiadczenia (S,A,R,S',T) przez `StateReporter (UDP)` do serwera (`ES`). Pobiera aktualne wagi (`.onnx`) z serwera (`WS`).
* **Monitoring:** Proces TensorBoard (`TBP`) wizualizuje metryki zapisane przez serwer RL (`TBW`).

---

# Slajd 4

## Jak Robot Siƒô Uczy? (Cykl Treningowy)

1. **Dzia≈Çanie:** Robot w Robocode obserwuje `GameState` i u≈ºywa swojej lokalnej sieci `NetDJL` (wagi z `.onnx`) do wyboru akcji.
2. **Do≈õwiadczenie:** Wykonuje akcjƒô w `Simulatorze`, obserwuje nowy `GameState` i otrzymuje nagrodƒô.
3. **Przes≈Çanie Danych:** `StateReporter` wysy≈Ça pakiet UDP z krotkƒÖ do≈õwiadczenia (S,A,R,S',T) do `Environment Server (ES)` na Serwerze RL.
4. **Nauka na Serwerze:** `ES` zapisuje dane w `Replay Memory (Mem)`. Gdy jest wystarczajƒÖco danych, `QNet` jest trenowana na pr√≥bce z `Mem` przez `Optimizera`.
5. **Aktualizacja Wiedzy Robota:** `QNet` jest okresowo zapisywana do pliku `.onnx`. `Weight Server (WS)` udostƒôpnia ten plik. Agent (`NetDJL`) pobiera go przez HTTP i aktualizuje swojƒÖ lokalnƒÖ sieƒá.

**Powtarzanie tego cyklu sprawia, ≈ºe `QNet` (i tym samym `NetDJL`) staje siƒô coraz lepsza w podejmowaniu decyzji.**

---

# Slajd 5

## Format Danych: Pakiet UDP Agent -> Serwer

Struktura pakietu wysy≈Çanego z `StateReporter` (Agent) do `Environment Server` (Serwer RL).

```mermaid
---
title: "Robocode Agent -> RL Server UDP Packet"
---
packet-beta
    0-31: "Client ID (int)"
    32-287: "Start State (8 * float32)"
    288-295: "Action (byte)"
    296-327: "Reward (float32)"
    328-583: "End State (8 * float32)"
    584-591: "Is Terminal (bool as byte: 0 or 1)"
```

---

# Slajd 6

## Opis Pakietu UDP

Pakiet zawiera komplet informacji o jednym kroku (przej≈õciu) wykonanym przez agenta.

* **Client ID (bity 0-31):** Unikalny identyfikator robota (int, 4 bajty).
* **Start State (bity 32-287):** Stan gry przed akcjƒÖ (8 * float32 = 32 bajty).
* **Action (bity 288-295):** Wykonana akcja (byte, 1 bajt).
* **Reward (bity 296-327):** Otrzymana nagroda (float32, 4 bajty).
* **End State (bity 328-583):** Stan gry po akcji (8 * float32 = 32 bajty).
* **Is Terminal (bity 584-591):** Czy stan ko≈Ñcowy? (byte, 1 bajt: 0=Nie, 1=Tak).

≈ÅƒÖcznie 74 bajty danych o przej≈õciu w ka≈ºdym pakiecie UDP.
